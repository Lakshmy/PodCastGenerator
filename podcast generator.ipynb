{"cells":[{"cell_type":"code","source":["# Install required packages\n","%pip install crewai==0.28.8 crewai_tools==0.1.6 langchain_community langchain-openai azure-identity python-dotenv pydub azure-cognitiveservices-speech --quiet"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fedc7b9d-6b97-4418-b225-6cccd9a3a873"},{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","# Set up Azure Open AI GPT-4o as the LLM to be used by the CrewAI Agents\n","\n","import os\n","import certifi\n","from langchain_openai import AzureChatOpenAI\n","from dotenv import load_dotenv\n","import base64\n","import tempfile\n","import time\n","import re # Import regex for SSML cleanup\n","from pathlib import Path\n","from crewai import Agent, Task, Crew\n","from crewai_tools import tool\n","from azure.cognitiveservices.speech import SpeechConfig, SpeechSynthesizer, AudioConfig, ResultReason, CancellationReason, SpeechSynthesisOutputFormat\n","from langchain.schema import HumanMessage, AIMessage\n","from datetime import datetime"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c744b89-dbae-4a68-b3d2-d5f878c2d7a4"},{"cell_type":"code","source":["\n","# ===================== CONFIGURABLE VARIABLES =====================\n","# --- Input Configuration ---\n","# Load environment variables\n","load_dotenv(dotenv_path=\"/lakehouse/default/Files/.env\") #Example: \"/lakehouse/default/Files/.env\"\n","# =================================================================="],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c8d6cd3-d3a1-4ea4-94c4-ab36649b7f52"},{"cell_type":"code","source":["# Set the environment variable 'REQUESTS_CA_BUNDLE' to the path of the certificate bundle provided by certifi\n","os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n","# Set the environment variable 'SSL_CERT_FILE' to the path of the certificate bundle provided by certifi\n","os.environ['SSL_CERT_FILE'] = certifi.where()\n","\n","# Print environment variables (remove in production)\n","print(\"Checking environment variables:\")\n","print(f\"Endpoint exists: {'MY_AZURE_OPENAI_ENDPOINT' in os.environ}\")\n","print(f\"API Key exists: {'AZURE_OPENAI_KEY' in os.environ}\")\n","print(f\"Deployment exists: {'AZURE_OPENAI_MODEL_DEPLOYMENT' in os.environ}\")\n","\n","# Get configuration\n","api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n","azure_endpoint = os.getenv(\"MY_AZURE_OPENAI_ENDPOINT\")\n","deployment_name = os.getenv(\"AZURE_OPENAI_MODEL_DEPLOYMENT\")\n","api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n","\n","# Print configurations (remove sensitive data in production)\n","print(\"\\nConfiguration values:\")\n","print(f\"Endpoint: {azure_endpoint}\")\n","print(f\"Deployment: {deployment_name}\")\n","print(f\"API Key length: {len(api_key) if api_key else 0}\")\n","print(f\"Api version: {api_version}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20fdb705-7f2a-46b7-8225-950631327772"},{"cell_type":"code","source":["# Initialize Azure OpenAI model\n","llm = AzureChatOpenAI(\n","    azure_endpoint=azure_endpoint,\n","    api_key=api_key,\n","    azure_deployment=deployment_name,\n","    api_version=api_version,\n",")\n","\n","# Test the connection\n","try:\n","    print(\"\\nTesting Azure OpenAI connection...\")\n","    response = llm.invoke(\"Hello! This is a test message.\")\n","    print(\"Connection successful!\")\n","    print(f\"Response: {response}\")\n","except Exception as e:\n","    print(f\"Error: {str(e)}\")\n","    print(f\"Error type: {type(e)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"81556669-75f2-4a7d-9349-144fdf93460e"},{"cell_type":"code","source":["# Assuming 'llm' is already initialized in your environment\n","global_llm = llm # Make sure llm is assigned here"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"54bda926-120e-4acd-b8ed-18b9820e7b14"},{"cell_type":"code","source":["# Azure Speech Service Configuration\n","# IMPORTANT: Ensure the environment variables AZURE_SPEECH_KEY and AZURE_SPEECH_REGION are set in your environment.\n","AZURE_SPEECH_ENDPOINT= os.getenv(\"AZURE_SPEECH_ENDPOINT\")\n","AZURE_SPEECH_KEY =  os.getenv(\"AZURE_SPEECH_KEY\") # e.g., set AZURE_SPEECH_KEY=\"your_azure_speech_key\" in the .env file\n","AZURE_SPEECH_REGION =  os.getenv(\"AZURE_SPEECH_REGION\") # e.g., set AZURE_SPEECH_REGION=\"yourazureregion\" in the .env file\n","AZURE_SPEECH_LANG = \"en-US\" # Default fallback\n","\n","# Voices for the podcast hosts\n","# Find available voice names here: https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts\n","AZURE_SPEECH_VOICE_1 = \"en-US-Emma2:DragonHDLatestNeural\" # Example: \"en-US-Emma2:DragonHDLatestNeural\" or \"en-AU-WilliamNeural\"\n","AZURE_SPEECH_VOICE_2 = \"en-US-Andrew3:DragonHDLatestNeural\"   # Example: \"en-US-Andrew3:DragonHDLatestNeural\" or \"en-AU-NatashaNeural\"\n","\n","# Derive language automatically from the first voice for SSML tag\n","try:\n","    # Check if AZURE_SPEECH_VOICE_1 is not None or empty before attempting split\n","    if AZURE_SPEECH_VOICE_1 and isinstance(AZURE_SPEECH_VOICE_1, str) and AZURE_SPEECH_VOICE_1.startswith('<'):\n","        print(f\"Warning: AZURE_SPEECH_VOICE_1 seems to contain a placeholder '{AZURE_SPEECH_VOICE_1}'. Using default language '{AZURE_SPEECH_LANG}'. Please replace the placeholder.\")\n","    elif AZURE_SPEECH_VOICE_1 and isinstance(AZURE_SPEECH_VOICE_1, str):\n","         parts = AZURE_SPEECH_VOICE_1.split('-')\n","         if len(parts) >= 2:\n","             AZURE_SPEECH_LANG = f\"{parts[0]}-{parts[1]}\"\n","         else:\n","              print(f\"Warning: Could not reliably determine language from voice '{AZURE_SPEECH_VOICE_1}'. Using default '{AZURE_SPEECH_LANG}'.\")\n","    elif AZURE_SPEECH_VOICE_1 is None:\n","         print(f\"Warning: AZURE_SPEECH_VOICE_1 is not set. Using default language '{AZURE_SPEECH_LANG}'.\")\n","\n","except Exception as e:\n","    print(f\"Warning: Error parsing voice name for language '{AZURE_SPEECH_VOICE_1}': {e}. Using default '{AZURE_SPEECH_LANG}'.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"730e0053-4bca-4363-bd24-f7cf2012490c"},{"cell_type":"code","source":["# Path to the image file to be processed within the Lakehouse environment\n","LOCAL_IMAGE_PATH = \"/lakehouse/default/Files/sales_summary.png\" #Example: \"/lakehouse/default/Files/podcastreport/store_incidents.png\"\n","# --- File and Credential Checks ---\n","print(f\"--- Configuration Summary ---\")\n","print(f\"Input Image Path: {LOCAL_IMAGE_PATH}\")\n","if not LOCAL_IMAGE_PATH or LOCAL_IMAGE_PATH.startswith(\"<\"):\n","    print(\"Warning: Input image path appears to be a placeholder or empty. Please update the LOCAL_IMAGE_PATH variable.\")\n","    # Optionally raise an error if you want to force the user to change it:\n","    # raise ValueError(\"Please replace the placeholder value for LOCAL_IMAGE_PATH before running.\")\n","elif not os.path.exists(LOCAL_IMAGE_PATH):\n","    print(f\"Error: Input image file NOT found at: {LOCAL_IMAGE_PATH}\")\n","    raise FileNotFoundError(f\"Could not find image at the specified LOCAL_IMAGE_PATH: {LOCAL_IMAGE_PATH}\")\n","else:\n","    print(\"Input image file found.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d7d11c06-d5ea-4631-87df-f80f182ab708"},{"cell_type":"code","source":["# Base name for the final output audio file ('.wav' will be appended automatically)\n","OUTPUT_FILENAME = \"sales_summary\" # Example: \"test_analysis_podcast\"\n","current_date = datetime.now().strftime(\"%Y%m%d\")\n","# Construct the file name\n","OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}_{current_date}\"\n","print(OUTPUT_FILENAME)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74fae22d-a4aa-46b5-a6e9-9b675b9f507a"},{"cell_type":"code","source":["\n","\n","# Directory path where the final audio file will be saved. Ensure this path exists or is writable. Must end with a forward slash '/'.\n","DEFAULT_OUTPUT_FILEPATH = \"/lakehouse/default/Files/podcastreport/\" # Example: \"/lakehouse/default/Files/podcastreport/\"\n","OUTPUT_FILENAME = \"sales_summary\" # todo - change to use the name of input image file\n","current_date = datetime.now().strftime(\"%Y%m%d\")\n","OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}_{current_date}\"\n","# Construct the full output file path\n","DEFAULT_OUTPUT_FILENAME = f\"{OUTPUT_FILENAME}.wav\"\n","OUTPUT_FILE_PATH = DEFAULT_OUTPUT_FILEPATH + DEFAULT_OUTPUT_FILENAME\n","\n","# MIME type mappings (Generally constant)\n","MIME_TYPES = {\n","    \".png\": \"image/png\",\n","    \".jpg\": \"image/jpeg\",\n","    \".jpeg\": \"image/jpeg\",\n","    \".gif\": \"image/gif\",\n","    \".webp\": \"image/webp\"\n","}\n","# =================================================================="],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"24ba76bd-58c7-4c83-aa0b-c119deb84422"},{"cell_type":"code","source":["\n","print(f\"Azure Speech Region: {AZURE_SPEECH_REGION}\")\n","if not AZURE_SPEECH_KEY:\n","    print(\"Warning: AZURE_SPEECH_KEY environment variable not found or empty.\")\n","    # Optionally raise an error\n","    # raise ValueError(\"Azure Speech Key (AZURE_SPEECH_KEY) environment variable not set.\")\n","if not AZURE_SPEECH_REGION:\n","    print(\"Warning: AZURE_SPEECH_REGION environment variable not found or empty.\")\n","    # Optionally raise an error\n","    # raise ValueError(\"Azure Speech Region (AZURE_SPEECH_REGION) environment variable not set.\")\n","\n","print(f\"Host 1 Voice: {AZURE_SPEECH_VOICE_1}\")\n","print(f\"Host 2 Voice: {AZURE_SPEECH_VOICE_2}\")\n","print(f\"Derived SSML Language: {AZURE_SPEECH_LANG}\")\n","print(f\"Output Base Filename: {OUTPUT_FILENAME}\")\n","print(f\"Output Directory: {DEFAULT_OUTPUT_FILEPATH}\")\n","print(f\"Full Output Path: {OUTPUT_FILE_PATH}\")\n","print(f\"----------------------------\")\n","\n","# Pre-flight check for output directory\n","output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n","if output_dir and not os.path.exists(output_dir):\n","    print(f\"Warning: Output directory '{output_dir}' does not exist. Attempting to create it.\")\n","    try:\n","        os.makedirs(output_dir, exist_ok=True) # Use exist_ok=True\n","        print(f\"Successfully created output directory: {output_dir}\")\n","    except OSError as e:\n","        print(f\"Error: Failed to create output directory '{output_dir}': {e}\")\n","        raise # Re-raise the error to stop execution if directory creation fails\n","# --- End Checks ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"023dff5a-abc3-4966-b8aa-8460ab8d1bf8"},{"cell_type":"code","source":["@tool(\"Image Description Tool\")\n","def describe_image(local_image_path: str) -> str:\n","    \"\"\"\n","    Analyzes and describes images in detail.\n","    Provide the full path to the image file you want to describe.\n","    \"\"\"\n","    # --- (describe_image tool) ---\n","    try:\n","        # Access the global LLM\n","        llm = global_llm\n","        if llm is None:\n","             return \"Error: LLM not provided to the tool. Make sure to set global_llm before using this tool.\"\n","        # Print debug info\n","        print(f\"Tool received path: {local_image_path}\")\n","        # Convert to Path object\n","        image_path = Path(local_image_path)\n","        # Double-check file exists\n","        if not image_path.exists():\n","            print(f\"Error: Image file not found at {image_path}\")\n","            return f\"Error: Image file not found at {image_path}\"\n","        else:\n","            print(f\"Tool confirmed file exists at {image_path}\")\n","        # Read and encode image to base64\n","        with open(image_path, \"rb\") as img_file:\n","            img_data = base64.b64encode(img_file.read()).decode(\"utf-8\")\n","            file_size = os.path.getsize(image_path)\n","            print(f\"Successfully read image file. Size: {file_size} bytes\")\n","        # Get the MIME type\n","        extension = image_path.suffix.lower()\n","        mime_type = MIME_TYPES.get(extension, \"application/octet-stream\")\n","        print(f\"Using MIME type: {mime_type}\")\n","        # Format image content for Azure OpenAI\n","        image_content = f\"data:{mime_type};base64,{img_data}\"\n","        # Use the image_content with Azure OpenAI\n","        print(\"Sending image to Azure OpenAI for description...\")\n","        try:\n","            # Create a message with the image\n","            message = HumanMessage(\n","                content=[\n","                    {\n","                        \"type\": \"text\",\n","                        \"text\": \"Please describe this image in detail, focusing on any text, charts, graphs, or data presented:\" # Slightly tuned prompt\n","                    },\n","                    {\n","                        \"type\": \"image_url\",\n","                        \"image_url\": {\n","                            \"url\": image_content\n","                        }\n","                    }\n","                ]\n","            )\n","            # Get the response from Azure OpenAI\n","            response = llm.invoke([message])\n","            print(\"Received response from Azure OpenAI\")\n","            # Extract the content from the response\n","            if isinstance(response, AIMessage):\n","                return response.content\n","            else:\n","                return str(response)\n","        except Exception as llm_error:\n","            print(f\"LLM error: {str(llm_error)}\")\n","            return f\"Error getting image description from LLM: {str(llm_error)}\"\n","    except Exception as e:\n","        print(f\"Error in tool: {str(e)}\")\n","        return f\"Error processing image: {str(e)}\"\n","    # --- (End of describe_image code) ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89fc4749-2d27-4cc3-a0d4-4d9b3e9e3b9b"},{"cell_type":"code","source":["@tool(\"SSML Formatter Tool\")\n","def format_ssml_tool(dialogue_content: str) -> str:\n","    \"\"\"\n","    Takes podcast dialogue text (expected to contain <voice> tags already)\n","    and wraps it in the necessary <speak> root element for Azure SSML,\n","    dynamically setting the xml:lang attribute based on the configured primary voice.\n","    It cleans up potential extra whitespace or markdown code fences.\n","    Provide the raw dialogue content generated by the scriptwriter.\n","    \"\"\"\n","    # --- (format_ssml_tool) ---\n","    print(\"SSML Formatter Tool: Received dialogue content.\")\n","    ssml_body = str(dialogue_content).strip()\n","\n","    # Remove potential markdown code fences added by LLM\n","    ssml_body = re.sub(r'^```xml\\s*', '', ssml_body, flags=re.IGNORECASE | re.MULTILINE)\n","    ssml_body = re.sub(r'\\s*```$', '', ssml_body, flags=re.MULTILINE)\n","    ssml_body = ssml_body.strip()\n","\n","    lang_attribute = f'xml:lang=\"{AZURE_SPEECH_LANG}\"'\n","    is_already_wrapped = ssml_body.startswith('<speak') and ssml_body.endswith('</speak>')\n","    has_correct_lang = lang_attribute in ssml_body[:150]\n","\n","    if is_already_wrapped and has_correct_lang:\n","        print(f\"SSML Formatter Tool: Input already correctly wrapped with {lang_attribute}.\")\n","        return ssml_body\n","    elif is_already_wrapped and not has_correct_lang:\n","        print(f\"SSML Formatter Tool: Input wrapped, but ensuring correct {lang_attribute}.\")\n","        pattern = re.compile(r'(<speak[^>]*)(\\s*xml:lang=\"[^\"]*\")?([^>]*>)', re.IGNORECASE)\n","        if pattern.search(ssml_body):\n","             ssml_body = pattern.sub(rf'\\1 {lang_attribute}\\3', ssml_body, count=1)\n","        else:\n","             ssml_body = ssml_body.replace('<speak', f'<speak {lang_attribute}', 1)\n","        return ssml_body\n","    else:\n","        print(f\"SSML Formatter Tool: Wrapping dialogue content with {lang_attribute}.\")\n","        ssml_full = f'<speak version=\"1.0\" xmlns=\"http://www.w3.org/2001/10/synthesis\" {lang_attribute}>\\n{ssml_body}\\n</speak>'\n","        return ssml_full\n","    # --- (End of format_ssml_tool code) ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71d7106b-857d-4a14-a2e4-904c6b01022d"},{"cell_type":"code","source":["@tool(\"SSML to Speech Tool\")\n","def ssml_to_speech(ssml_input: str) -> str:\n","    \"\"\"\n","    Converts a **complete and valid** SSML string (including the root <speak> tag\n","    with correct xml:lang, and <voice> tags defining multiple voices) into speech\n","    and saves it as a single WAV file. Provide the full, validated SSML string.\n","    Use the 'SSML Formatter Tool' first to ensure validity.\n","    \"\"\"\n","    # --- (Keep the ssml_to_speech tool code as corrected previously) ---\n","    # ... (code from previous version for ssml_to_speech) ...\n","    try:\n","        # Credentials check\n","        speech_config = SpeechConfig(subscription=AZURE_SPEECH_KEY, region=AZURE_SPEECH_REGION)\n","        speech_config.set_speech_synthesis_output_format(SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm)\n","\n","        ssml_content = str(ssml_input)\n","\n","        if not ssml_content.strip().startswith('<speak'):\n","            print(f\"Warning in ssml_to_speech: Input SSML might be invalid (missing <speak> tag): {ssml_content[:100]}...\")\n","\n","        print(f\"SSML to Speech Tool: Processing SSML (language should be embedded within): '{ssml_content[:150]}...'\")\n","        print(f\"Output will be saved to: {OUTPUT_FILE_PATH}\")\n","\n","        if not ssml_content:\n","            return \"Error: No SSML content provided for speech synthesis.\"\n","\n","        output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n","        if output_dir and not os.path.exists(output_dir):\n","            os.makedirs(output_dir); print(f\"Created output directory: {output_dir}\")\n","\n","        audio_config = AudioConfig(filename=OUTPUT_FILE_PATH)\n","        synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n","\n","        print(\"Synthesizing SSML...\")\n","        result = synthesizer.speak_ssml_async(ssml_content).get()\n","\n","        del synthesizer; del audio_config; time.sleep(0.1)\n","\n","        if result.reason == ResultReason.SynthesizingAudioCompleted:\n","            wait_time = 0\n","            while not (os.path.exists(OUTPUT_FILE_PATH) and os.path.getsize(OUTPUT_FILE_PATH) > 0) and wait_time < 5:\n","                time.sleep(0.5); wait_time += 0.5\n","            if not (os.path.exists(OUTPUT_FILE_PATH) and os.path.getsize(OUTPUT_FILE_PATH) > 0):\n","                 return f\"Error: Output file {OUTPUT_FILE_PATH} is missing or empty after synthesis completed.\"\n","            else:\n","                 print(f\"SSML synthesis successful. File saved to {OUTPUT_FILE_PATH} ({os.path.getsize(OUTPUT_FILE_PATH)} bytes).\")\n","                 return f\"Speech synthesis successful. File saved at: {OUTPUT_FILE_PATH}\"\n","        elif result.reason == ResultReason.Canceled:\n","            cancellation_details = result.cancellation_details\n","            error_message = f\"SSML synthesis canceled. Reason: {cancellation_details.reason}\"\n","            if cancellation_details.reason == CancellationReason.Error:\n","                print(f\"Cancellation Error Code: {cancellation_details.error_code}\")\n","                print(f\"Cancellation Error Details: {cancellation_details.error_details}\")\n","                error_message += f\" Error details: {cancellation_details.error_details}\"\n","            print(error_message)\n","            if os.path.exists(OUTPUT_FILE_PATH):\n","                try: os.remove(OUTPUT_FILE_PATH)\n","                except OSError as remove_err: print(f\"Warning: Could not remove partially created file {OUTPUT_FILE_PATH}: {remove_err}\")\n","            return error_message\n","        else:\n","             error_message = f\"Unexpected SSML synthesis result: {result.reason}\"\n","             print(error_message)\n","             if os.path.exists(OUTPUT_FILE_PATH):\n","                 try: os.remove(OUTPUT_FILE_PATH)\n","                 except OSError as remove_err: print(f\"Warning: Could not remove potentially invalid file {OUTPUT_FILE_PATH}: {remove_err}\")\n","             return error_message\n","\n","    except Exception as e:\n","        import traceback\n","        print(f\"Error in ssml_to_speech function: {str(e)}\")\n","        print(traceback.format_exc())\n","        if os.path.exists(OUTPUT_FILE_PATH):\n","            try: os.remove(OUTPUT_FILE_PATH)\n","            except OSError as remove_err: print(f\"Warning: Could not remove file {OUTPUT_FILE_PATH} during exception handling: {remove_err}\")\n","        return f\"Error during SSML to speech processing: {str(e)}\"\n","    # --- (End of ssml_to_speech code) ---"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be164406-4ed4-42d8-9fdb-c87648b5f17e"},{"cell_type":"code","source":["# ======================= AGENTS =======================\n","\n","image_analyst = Agent(\n","    role=\"Image Analyst\",\n","    goal=\"Provide comprehensive and objective textual descriptions of images, capturing all relevant visual details, especially focusing on text, charts, graphs, or data presented.\",\n","    backstory=\"You're an expert at analyzing and describing visual content with meticulous attention to detail, skilled at extracting factual information from images.\",\n","    verbose=True, allow_delegation=False, llm=global_llm, tools=[describe_image]\n",")\n","\n","# --- NEW Report Analyzer Agent ---\n","report_analyzer = Agent(\n","    role=\"Business Report Analyzer\",\n","    goal=(\n","        \"Analyze the provided detailed image description (containing text, charts, data etc.) \"\n","        \"to extract key business insights, trends, figures, and potential implications. \"\n","        \"Synthesize these findings into a structured written analysis presented as an essay, \"\n","        \"starting with a concise Executive Summary.\"\n","        ),\n","    backstory=(\n","        \"You are a meticulous business analyst skilled at interpreting data presented visually (via its textual description). \"\n","        \"You excel at identifying significant patterns, summarizing complex information clearly, \"\n","        \"and presenting actionable findings in a well-structured report format suitable for executive review and communication planning.\"\n","        ),\n","    verbose=True,\n","    allow_delegation=False,\n","    llm=global_llm\n","    # This agent primarily works on text context, no specific tool needed beyond LLM capabilities.\n",")\n","\n","# --- Podcast Writer Agent (Now takes analysis as input) ---\n","podcast_writer = Agent(\n","    role=\"Podcast Dialogue Creator\",\n","    goal=(\n","        \"My main goal is to take that detailed business analysis report and turn it into a friendly, engaging chat between two podcast hosts. \"\n","        \"I'll write the script so it sounds natural, making sure to tag each host's lines correctly (using `<voice name='...'>` tags with the specific voices: \"\n","        f\"'{AZURE_SPEECH_VOICE_1}' for Host 1 and '{AZURE_SPEECH_VOICE_2}' for Host 2) so the final audio sounds great. \"\n","        \"I'll also do my best to wrap the whole thing in the main `<speak>` tags needed for the audio generation step.\"\n","    ),\n","    backstory=(\n","        \"Think of me as your creative partner who's great at taking serious reports and making them easy to understand and interesting to listen to. \"\n","        \"I specialize in writing natural-sounding conversations for two people, making sure all the important points from the analysis are covered clearly. \"\n","        \"I know how to set up the script with the right formatting (like those `<voice>` tags) so it's ready for the next step of actually creating the audio.\"\n","    ),\n","    verbose=True, allow_delegation=False, llm=global_llm\n",")\n","\n","# --- Speech Synthesizer Agent (Keep as before) ---\n","speech_synthesizer = Agent(\n","    role=\"SSML Formatting and Speech Synthesis Orchestrator\",\n","    goal=(\n","        \"Take raw podcast dialogue, format it into valid SSML using the SSML Formatter Tool (which sets the correct language based on configuration), \"\n","        \"and then convert the finalized SSML into a single high-quality spoken audio file using the SSML to Speech Tool.\"\n","        ),\n","    backstory=(\n","        \"You are an expert workflow manager for audio production. You first ensure the script is perfectly formatted as SSML with the correct language, \"\n","        \"then you use Azure's Text-to-Speech service to generate seamless, multi-voice audio output from the validated SSML.\"\n","        ),\n","    verbose=True, llm=global_llm, tools=[format_ssml_tool, ssml_to_speech]\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78a3b89e-8b67-4ced-98c4-d3e72753581b"},{"cell_type":"code","source":["# ======================= TASKS =======================\n","\n","image_task = Task(\n","    description=(\n","        f\"Analyze the image located at {LOCAL_IMAGE_PATH}. Provide a comprehensive and objective textual description \"\n","        \"detailing all visible elements, scenes, objects, text, charts, graphs, and any discernible context or data presented visually. \"\n","        \"This description will be used by the Business Report Analyzer.\"\n","    ),\n","    agent=image_analyst,\n","    expected_output=\"A detailed, objective textual description of the image's contents, focusing on data and textual elements.\"\n",")\n","\n","# --- Analysis Task ---\n","analysis_task = Task(\n","    description=(\n","        \"Review the detailed image description provided in the context. \"\n","        \"Based *solely* on the information presented in that description (text, data points, chart descriptions etc.), \"\n","        \"perform a thorough business analysis. Identify key insights, significant trends, important figures, and potential implications. \"\n","        \"Structure your findings as a written essay. Start with a clear 'Executive Summary' section summarizing the main points, \"\n","        \"followed by a more detailed 'Analysis' section elaborating on the findings. \"\n","        \"This report will be used to create a podcast script.\"\n","    ),\n","    agent=report_analyzer,\n","    expected_output=(\n","        \"A well-structured written analysis in essay format. \"\n","        \"It MUST begin with an 'Executive Summary' section. \"\n","        \"It MUST be followed by a detailed 'Analysis' section. \"\n","        \"The analysis must be based ONLY on the information from the input image description.\"\n","    ),\n","    context=[image_task] # Depends on the image description\n",")\n","\n","# --- Business Update Task (it depends on analysis_task) ---\n","business_update_task = Task(\n","    description=(\n","        \"Okay, I have the business analysis report here (check the context). My task is to translate the key findings and insights from this report \"\n","        \"into a natural, back-and-forth dialogue for a podcast featuring Host 1 and Host 2. The conversation should flow well and make the analysis easy for listeners to grasp. \"\n","        \"\\n\\n**Formatting Guide for Audio:**\\n\" # Use markdown for emphasis\n","        f\"*   **Host 1:** Use the `<voice name=\\\"{AZURE_SPEECH_VOICE_1}\\\">...</voice>` tag for everything Host 1 says.\\n\"\n","        f\"*   **Host 2:** Use the `<voice name=\\\"{AZURE_SPEECH_VOICE_2}\\\">...</voice>` tag for everything Host 2 says.\\n\"\n","        \"*   **Structure:** Please alternate between these tags as the hosts speak. Getting these voice tags right is crucial for the audio step!\\n\"\n","        \"*   **Wrapping (Optional but helpful):** If you can, please also wrap the entire dialogue within the main `<speak>...</speak>` tags.\\n\"\n","        \"*   **Content:** Just include the words the hosts will actually say. No extra labels like 'Host 1:', notes about music, or markdown formatting around the SSML itself.\"\n","    ),\n","    agent=podcast_writer,\n","    expected_output=(\n","         \"A complete podcast script formatted as dialogue. It should feature alternating \"\n","         f\"`<voice name='{AZURE_SPEECH_VOICE_1}'>...</voice>` and `<voice name='{AZURE_SPEECH_VOICE_2}'>...</voice>` tags \"\n","         \"containing the spoken lines for each host, accurately reflecting the input analysis in a conversational style. \"\n","         \"Ideally, the whole script will be enclosed in `<speak>...</speak>` tags.\"\n","    ),\n","    context=[analysis_task] # Depends on the analysis report\n",")\n","\n","# --- Speech Task (it depends on business_update_task) ---\n","speech_task = Task(\n","    description=(\n","        \"Process the podcast dialogue script received from the previous task (available in context). \"\n","        \"Step 1: Use the 'SSML Formatter Tool' to ensure the script is wrapped in valid `<speak>...</speak>` tags with the correct language attribute (derived automatically from config). \"\n","        \"Step 2: Take the **output** from the 'SSML Formatter Tool' and use the 'SSML to Speech Tool' to synthesize this final, validated SSML into a single audio track. \"\n","        f\"Ensure the final audio is saved correctly as a .wav file to the path '{OUTPUT_FILE_PATH}'. \"\n","        \"Report success or failure, including the output path on success or detailed error messages on failure.\"\n","    ),\n","    agent=speech_synthesizer,\n","    expected_output=(\n","        \"Confirmation of successful audio file generation from the formatted SSML input, including the \"\n","        f\"file path where the .wav file was saved (expected: {OUTPUT_FILE_PATH}), or a descriptive error message if any step failed.\"\n","    ),\n","    context=[business_update_task] # Depends on the SSML script\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5b12341d-0d08-422f-aabe-9fc3a64582e1"},{"cell_type":"code","source":["# ======================= CREW EXECUTION =======================\n","if __name__ == \"__main__\":\n","    # Make sure global_llm is set\n","    if 'global_llm' not in globals() or global_llm is None:\n","        raise ValueError(\"The 'llm' variable is not defined or not assigned to 'global_llm'. Please ensure your LLM is initialized.\")\n","\n","    print(f\"\\n--- Starting Crew Workflow ---\")\n","    print(f\"Image Path: {LOCAL_IMAGE_PATH}\")\n","    print(f\"Output Audio Path: {OUTPUT_FILE_PATH}\")\n","    print(f\"Host 1 Voice (Azure): {AZURE_SPEECH_VOICE_1}\")\n","    print(f\"Host 2 Voice (Azure): {AZURE_SPEECH_VOICE_2}\")\n","    print(f\"--> Derived SSML Language: {AZURE_SPEECH_LANG}\")\n","    print(f\"----------------------------\\n\")\n","\n","    # --- Updated Crew Definition ---\n","    crew = Crew(\n","        agents=[\n","            image_analyst,\n","            report_analyzer, # Added agent\n","            podcast_writer,\n","            speech_synthesizer\n","        ],\n","        tasks=[\n","            image_task,\n","            analysis_task, # Added task\n","            business_update_task,\n","            speech_task\n","        ],\n","        verbose=2 # Use verbose=2 for detailed logs\n","    )\n","    # --- End Updated Crew Definition ---\n","\n","    # Execute the crew workflow\n","    print(\"\\n--- Kicking off Crew ---\")\n","    result = crew.kickoff()\n","    print(\"\\n--- Crew Workflow Finished ---\")\n","    print(\"Final result from Crew:\", result)\n","\n","    # --- Post-Run Verification (Keep as before) ---\n","    print(f\"\\n--- Post-Run Verification ---\")\n","    if os.path.exists(OUTPUT_FILE_PATH):\n","        print(f\"Success: Output audio file found at: {OUTPUT_FILE_PATH}\")\n","        print(f\"File size: {os.path.getsize(OUTPUT_FILE_PATH)} bytes\")\n","    else:\n","        print(f\"Error: Output audio file NOT found at the expected path: {OUTPUT_FILE_PATH}\")\n","        print(\"Please check the logs above for errors during the analysis, formatting or synthesis tasks.\")\n","    print(f\"---------------------------\\n\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c9412df-a7d0-4a76-a04e-c1d453899779"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"5f9213df-9ace-4095-b44a-341ae233c2e5","known_lakehouses":[{"id":"5f9213df-9ace-4095-b44a-341ae233c2e5"}],"default_lakehouse_name":"wwilakehouse","default_lakehouse_workspace_id":"f378d61a-e81c-4f4f-8040-e383da4436c8"}}},"nbformat":4,"nbformat_minor":5}